<html>
<head></head>
<body>
<center>
<h1>History of Artificial Intelligence</h1></center>
<marquee>AI IS NEW ERA</marquee>
<h2 id="c_1">CONTENTS</h2>
<ul>
<li><h2><a href="#chap_1">Birth of artificial intelligence (1941-56)</a></h2></li>
<li><h2><a href="#chap_2">First AI Winter (1974–1980)</a></h2></li>
<li><h2><a href="#chap_3">Big data, deep learning, AGI (2005–2017)</a></h2></li>
<li><h2><a href="#chap_4">Large language models, AI boom (2020–present)</a></h2></li>
<li><h2><a href="#contact_info">Contact Information</a></h2></li>
<li><h2><a href="#about_us">About Us</a></h2></li>
<li><h2><a href="#images">Images</a></h2></li>
</ul>
<h3 id="chap_1"></a>Birth of artificial intelligence<h3>
<p>The earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 1930s, 1940s, and early 1950s. Recent research in neurology had shown that the brain was an electrical network of neurons that fired in all-or-nothing pulses. Norbert Wiener's cybernetics described control and stability in electrical networks. Claude Shannon's information theory described digital signals (i.e., all-or-nothing signals). Alan Turing's theory of computation showed that any form of computation could be described digitally. The close relationship between these ideas suggested that it might be possible to construct an "electronic brain".

In the 1940s and 50s, a handful of scientists from a variety of fields (mathematics, psychology, engineering, economics and political science) explored several research directions that would be vital to later AI research.[59] Alan Turing was among the first people to seriously investigate the theoretical possibility of "machine intelligence".[60] The field of "artificial intelligence research" was founded as an academic discipline in 1956.[61]


Turing test[62]
Turing Test
Main article: Turing test
In 1950 Turing published a landmark paper "Computing Machinery and Intelligence", in which he speculated about the possibility of creating machines that think.[63][b] In the paper, he noted that "thinking" is difficult to define and devised his famous Turing Test: If a machine could carry on a conversation (over a teleprinter) that was indistinguishable from a conversation with a human being, then it was reasonable to say that the machine was "thinking".[64] This simplified version of the problem allowed Turing to argue convincingly that a "thinking machine" was at least plausible and the paper answered all the most common objections to the proposition.[65] The Turing Test was the first serious proposal in the philosophy of artificial intelligence.

Neuroscience and Hebbian theory
Donald Hebb was a Canadian psychologist whose work laid the foundation for modern neuroscience, particularly in understanding learning, memory, and neural plasticity. His most influential book, The Organization of Behavior (1949), introduced the concept of Hebbian learning, often summarized as "cells that fire together wire together." [66]

Hebb began formulating the foundational ideas for this book in the early 1940s, particularly during his time at the Yerkes Laboratories of Primate Biology from 1942 to 1947. He made extensive notes between June 1944 and March 1945 and sent a complete draft to his mentor Karl Lashley in 1946. The manuscript for The Organization of Behavior wasn’t published until 1949. The delay was due to various factors, including World War II and shifts in academic focus. By the time it was published, several of his peers had already published related ideas, making Hebb’s work seem less groundbreaking at first glance. However, his synthesis of psychological and neurophysiological principles became a cornerstone of neuroscience and machine learning. [67] [68]

Artificial neural networks
Walter Pitts and Warren McCulloch analyzed networks of idealized artificial neurons and showed how they might perform simple logical functions in 1943. They were the first to describe what later researchers would call a neural network.[69] The paper was influenced by Turing's paper 'On Computable Numbers' from 1936 using similar two-state boolean 'neurons', but was the first to apply it to neuronal function.[60] One of the students inspired by Pitts and McCulloch was Marvin Minsky who was a 24-year-old graduate student at the time. In 1951 Minsky and Dean Edmonds built the first neural net machine, the SNARC.[70] Minsky would later become one of the most important leaders and innovators in AI.

Cybernetic robots
Experimental robots such as W. Grey Walter's turtles and the Johns Hopkins Beast, were built in the 1950s. These machines did not use computers, digital electronics or symbolic reasoning; they were controlled entirely by analog circuitry.[71]

Game AI
In 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program[72] and Dietrich Prinz wrote one for chess.[73] Arthur Samuel's checkers program, the subject of his 1959 paper "Some Studies in Machine Learning Using the Game of Checkers", eventually achieved sufficient skill to challenge a respectable amateur.[74] Samuel's program was among the first uses of what would later be called machine learning.[75] Game AI would continue to be used as a measure of progress in AI throughout its history.</p>


<h3><a href="#c_1">BACK</a></h3>
<h3 id="chap_2">First AI Winter (1974–1980)</h3>


<p>The agencies which funded AI research, such as the British government, DARPA and the National Research Council (NRC) became frustrated with the lack of progress and eventually cut off almost all funding for undirected AI research. The pattern began in 1966 when the Automatic Language Processing Advisory Committee (ALPAC) report criticized machine translation efforts. After spending $20 million, the NRC ended all support.[157] In 1973, the Lighthill report on the state of AI research in the UK criticized the failure of AI to achieve its "grandiose objectives" and led to the dismantling of AI research in that country.[158] (The report specifically mentioned the combinatorial explosion problem as a reason for AI's failings.)[142][146][s] DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at CMU and canceled an annual grant of $3 million.[160][t]

Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues. "Many researchers were caught up in a web of increasing exaggeration."[161][u] However, there was another issue: since the passage of the Mansfield Amendment in 1969, DARPA had been under increasing pressure to fund "mission-oriented direct research, rather than basic undirected research". Funding for the creative, freewheeling exploration that had gone on in the 60s would not come from DARPA, which instead directed money at specific projects with clear objectives, such as autonomous tanks and battle management systems.[162][v]

The major laboratories (MIT, Stanford, CMU and Edinburgh) had been receiving generous support from their governments, and when it was withdrawn, these were the only places that were seriously impacted by the budget cuts. The thousands of researchers outside these institutions and the many more thousands that were joining the field were unaffected.[143]

Philosophical and ethical critiques
See also: Philosophy of artificial intelligence
Several philosophers had strong objections to the claims being made by AI researchers. One of the earliest was John Lucas, who argued that Gödel's incompleteness theorem showed that a formal system (such as a computer program) could never see the truth of certain statements, while a human being could.[164] Hubert Dreyfus ridiculed the broken promises of the 1960s and critiqued the assumptions of AI, arguing that human reasoning actually involved very little "symbol processing" and a great deal of embodied, instinctive, unconscious "know how".[w][166] John Searle's Chinese Room argument, presented in 1980, attempted to show that a program could not be said to "understand" the symbols that it uses (a quality called "intentionality"). If the symbols have no meaning for the machine, Searle argued, then the machine can not be described as "thinking".[167]

These critiques were not taken seriously by AI researchers. Problems like intractability and commonsense knowledge seemed much more immediate and serious. It was unclear what difference "know how" or "intentionality" made to an actual computer program. MIT's Minsky said of Dreyfus and Searle "they misunderstand, and should be ignored."[168] Dreyfus, who also taught at MIT, was given a cold shoulder: he later said that AI researchers "dared not be seen having lunch with me."[169] Joseph Weizenbaum, the author of ELIZA, was also an outspoken critic of Dreyfus' positions, but he "deliberately made it plain that [his AI colleagues' treatment of Dreyfus] was not the way to treat a human being,"[x] and was unprofessional and childish.[171]

Weizenbaum began to have serious ethical doubts about AI when Kenneth Colby wrote a "computer program which can conduct psychotherapeutic dialogue" based on ELIZA.[172][173][y] Weizenbaum was disturbed that Colby saw a mindless program as a serious therapeutic tool. A feud began, and the situation was not helped when Colby did not credit Weizenbaum for his contribution to the program. In 1976, Weizenbaum published Computer Power and Human Reason which argued that the misuse of artificial intelligence has the potential to devalue human life.[175]

Logic at Stanford, CMU and Edinburgh
Logic was introduced into AI research as early as 1958, by John McCarthy in his Advice Taker proposal.[176][101] In 1963, J. Alan Robinson had discovered a simple method to implement deduction on computers, the resolution and unification algorithm.[101] However, straightforward implementations, like those attempted by McCarthy and his students in the late 1960s, were especially intractable: the programs required astronomical numbers of steps to prove simple theorems.[176][177] A more fruitful approach to logic was developed in the 1970s by Robert Kowalski at the University of Edinburgh, and soon this led to the collaboration with French researchers Alain Colmerauer and Philippe Roussel [fr] who created the successful logic programming language Prolog.[178] Prolog uses a subset of logic (Horn clauses, closely related to "rules" and "production rules") that permit tractable computation. Rules would continue to be influential, providing a foundation for Edward Feigenbaum's expert systems and the continuing work by Allen Newell and Herbert A. Simon that would lead to Soar and their unified theories of cognition</p>


<button><a href="#c_1">BACK</a></button>
<h3 id="chap_3">Big data, deep learning, AGI (2005–2017)<h3>
<p>In the first decades of the 21st century, access to large amounts of data (known as "big data"), cheaper and faster computers and advanced machine learning techniques were successfully applied to many problems throughout the economy. A turning point was the success of deep learning around 2012 which improved the performance of machine learning on many tasks, including image and video processing, text analysis, and speech recognition.[258] Investment in AI increased along with its capabilities, and by 2016, the market for AI-related products, hardware, and software reached more than $8 billion, and the New York Times reported that interest in AI had reached a "frenzy".[259]

In 2002, Ben Goertzel and others became concerned that AI had largely abandoned its original goal of producing versatile, fully intelligent machines, and argued in favor of more direct research into artificial general intelligence. By the mid-2010s several companies and institutions had been founded to pursue Artificial General Intelligence (AGI), such as OpenAI and Google's DeepMind. During the same period, new insights into superintelligence raised concerns that AI was an existential threat. The risks and unintended consequences of AI technology became an area of serious academic research after 2016.</p>





<h3><a href="#c_1">BACK</a></h3>
<h3 id="chap_4">Large language models, AI boom (2020–present)</h3>




<p>The AI boom started with the initial development of key architectures and algorithms such as the transformer architecture in 2017, leading to the scaling and development of large language models exhibiting human-like traits of knowledge, attention and creativity. The new AI era began since 2020, with the public release of scaled large language models (LLMs) such as ChatGPT.[285]

Transformer architecture and large language models
Main article: Large language models
In 2017, the transformer architecture was proposed by Google researchers. It exploits an attention mechanism and became widely used in large language models.[286]

Large language models, based on the transformer, were developed by AGI companies: OpenAI released GPT-3 in 2020, and DeepMind released Gato in 2022. These are foundation models: they are trained on vast quantities of unlabeled data and can be adapted to a wide range of downstream tasks.[citation needed]

These models can discuss a huge number of topics and display general knowledge. The question naturally arises: are these models an example of artificial general intelligence? Bill Gates was skeptical of the new technology and the hype that surrounded AGI. However, Altman presented him with a live demo of ChatGPT4 passing an advanced biology test. Gates was convinced.[282] In 2023, Microsoft Research tested the model with a large variety of tasks, and concluded that "it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system".[287]

In 2024, OpenAI o3, a type of advanced reasoning model developed by OpenAI was announced. On the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) benchmark developed by François Chollet in 2019, the model achieved an unofficial score of 87.5% on the semi-private test, surpassing the typical human score of 84%. The benchmark is supposed to be a necessary, but not sufficient test for AGI. Speaking of the benchmark, Chollet has said "You’ll know AGI is here when the exercise of creating tasks that are easy for regular humans but hard for AI becomes simply impossible."[288]

AI boom
Main article: AI boom
Investment in AI grew exponentially after 2020, with venture capital funding for generative AI companies increasing dramatically. Total AI investments rose from $18 billion in 2014 to $119 billion in 2021, with generative AI accounting for approximately 30% of investments by 2023.[289] According to metrics from 2017 to 2021, the United States outranked the rest of the world in terms of venture capital funding, number of startups, and AI patents granted.[290] The commercial AI scene became dominated by American Big Tech companies, whose investments in this area surpassed those from U.S.-based venture capitalists.[291] OpenAI's valuation reached $86 billion by early 2024,[292] while NVIDIA's market capitalization surpassed $3.3 trillion by mid-2024, making it the world's largest company by market capitalization as the demand for AI-capable GPUs surged.[293]

15.ai, launched in March 2020[294] by an anonymous MIT researcher,[295][296] was one of the earliest examples of generative AI gaining widespread public attention during the initial stages of the AI boom.[297] The free web application demonstrated.</p>
<h3><a href="#c_1">BACK</a></h3>
<h3 id="contact_info">Contact Information</h3>
<p>prem nikhil phD, hyderabad, 76750744580</p>
<h3 id="about_us">About Us</h3>
<p>dbfjgvrgsiohfrigvlihsrgbviahrigasbfyatwgkgawgbfawgiygwagiqagwyfubwavawlhfawygfgawkfvawfbgawgrqwgyawkfbawgha</p>
<h3 id="images">Images</h3>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/eureka_zoomout.mp4"</a>robot AI</p>

</body>
</html>
